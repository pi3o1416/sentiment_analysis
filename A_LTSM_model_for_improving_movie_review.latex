\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{authblk}


\title{A LTSM model for improving movie review}

\author[1]{Monir Hossain}
\author[2]{Rafi Md. hasib}
\author[3]{T. M. Sazzad Hossain}
\author[4]{Md Habibur Rahman}
\affil[1]{ID : 2017-1-60-18, East West University}
\affil[2]{ID : 2017-1-60-138, East West University}
\affil[3]{ID : 2015-1-60-152, East West University}
\affil[4]{ID : 2015-1-60-142, East West University}

\begin{document}
\maketitle



\section{Introduction}

Natural language processing(NLP) is one of the interesting sectors in machine learning research. NLP is highly applicable in two types of problems: the first type is various computation tasks like spell checking, grammar checking, language translation etc . It is also highly used in reorganization of psychological and theoretical knowledge. It determines peopleâ€™s liking of a topic, event, issues etc . Everyday social networks and other sites generate more and more information about their views on political or any social topic. NLP can play a crucial role in Understanding people's views on a topic.

We proposed a LTSM model to find a better solution to this problem. We utilize LTSM(Long term memory concept) of Recurrent Neural Network to understand long term dependencies of word sequences. For word embedding we use a pre trained Word2vector model GloVe. For experiment we use \href{https://ai.stanford.edu/~amaas/data/sentiment/}{movie review data-set}
 from stanford University Our main contribution in the study as follows
 \begin{enumerate}
     \item Create word embedding from pre trained word2vector model glove 6b 200d
     \item To understand word sequence and their relation we use a sequential model LTSM to understand deeper semantic of words. 
 \end{enumerate}

Some of the work related traditionally scientists proposed a support vector machine, random forest, Naive Bayes to solve sentiment analysis problems which achieve better solutions. Later researchers applied a variant of Naive Bayes which resulted in a better solution than previous. Later researchers proposed a paragraph2vec approach for feature extraction which outperformed most of the TF, TF-IDF, SVM, NB models based on accuracy. Letter a new proposal was to solve Image based sentiment analysis problem using CNN framework approach. Which was experimented on a dataset with 1269 images which are collected from twitter. This system places high performance in terms of accuracy and precision. 



\section{Methodology}

In this section we describe detail of out proposed model. Initially we feed our LTSM model a word embedding to understand long term dependency between the sequence of word and and in the end of LTSM we apply a classifier layer to output our desire result.

Our LTSM layer is combined of Embedding layer ,LTSM layer , Dropout technique, Regularization, Danselayer and for activation RELU and sigmoid activation function.
\begin{enumerate}
    \item Embedding Layer : We use word2vec model for word embedding. we use pre trained glove word2vec model for word embedding
    \item LTSM layer : We use a LTSM layer after embedding layer
    \item Dropout layer : We use a dropout layer with keep\textunderscore prob=0.2
    \item Dense layer : After dropout layer we use a dense layer with 256 neuron , l2 regularization and RELU activation.
    \item Dense layer : Again a Dropout layer with 128 neuron , l2 regularization and RELU activation
    \item Dropout layer : After dense layer a dropout layer with keep \textunderscore prob = 0.2
    \item Finally a sigmoid activation layer for classifing the output. 
    
\end{enumerate}

\section{Experiment}
for experiment we use IMDBmovie review dataset form \href{https://ai.stanford.edu/~amaas/data/sentiment/}{stanford}. this dataset contain 50 thousand movie review with positive and negative review. we divide the dataset into 80:20 for train and test dataset. 
For model architecture we use keras library from tensorflow and glove model for word embedding. we also use natural language processing library nltk for processing data. after 30 epoch we get a 90 percent accuracy on test set. 

\section{Conclusion}
Our model is successfully able to classify user review with 90 present accuracy.
However it still have a high variance problem. so with hydrometer tuning it is possible to get an accuracy more than that. In future we will try to reduce high variance problem and we also try to combine CNN with LTSM to create a hybrid model which might result a good solution. 

\end{document} 
